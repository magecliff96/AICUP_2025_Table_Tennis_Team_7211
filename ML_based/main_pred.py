import pandas as pd
import numpy as np
import os
from sklearn.decomposition import PCA
from tqdm import tqdm
import xgboost as xgb
from collections import Counter
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.metrics import roc_auc_score, make_scorer

def multiclass_roc_auc_micro(y_true, y_pred_proba):
    """Micro-average ROC AUC scorer for multiclass"""
    return roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='micro')

# è½‰æˆ sklearn ç”¨çš„ scorer
micro_auc_scorer = make_scorer(multiclass_roc_auc_micro, needs_proba=True)

data_dir = 'avg_cycles_txt'
sensor_names = ['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']

# Step 1: å…ˆæ”¶é›†æ¯å€‹æ„Ÿæ¸¬å™¨çš„æ‰€æœ‰è³‡æ–™
sensor_data = {name: [] for name in sensor_names}
file_ids = []

for fname in tqdm(os.listdir(data_dir)):
    if not fname.endswith('.txt'):
        continue
    uid = fname.replace('.txt', '')
    file_ids.append(uid)
    
    data = np.loadtxt(os.path.join(data_dir, fname))  # shape: (173, 6)
    for i, name in enumerate(sensor_names):
        sensor_data[name].append(data[:, i])  # shape: (173,)

n_components = 3

# Step 2: å°æ¯å€‹æ„Ÿæ¸¬å™¨åš PCAï¼ˆfit æ‰€æœ‰ sampleï¼‰
sensor_pca = {}
for name in sensor_names:
    X = np.array(sensor_data[name])  # shape: (N_samples, 173)
    pca = PCA(n_components=n_components)
    pca.fit(X)
    sensor_pca[name] = pca

# Step 3: å°æ¯ç­†è³‡æ–™åš transform â†’ å¾—åˆ° 18 ç¶­ç‰¹å¾µ
features = []

for idx, uid in enumerate(file_ids):
    feature_row = []
    for i, name in enumerate(sensor_names):
        vec = sensor_data[name][idx].reshape(1, -1)  # shape: (1, 173)
        reduced = sensor_pca[name].transform(vec)[0]  # shape: (3,)
        feature_row.extend(reduced)
    features.append([uid] + feature_row)

# Step 4: æ”¾é€² DataFrameï¼Œåˆä½µå› training_df
columns = ['unique_id'] + [f'{name}_pca_{i}' for name in sensor_names for i in range(n_components)]
feature_df = pd.DataFrame(features, columns=columns)
feature_df.to_csv("pca_feature.csv", index = None)

# feature_df = pd.read_csv("pca_feature.csv")
feature_df['unique_id'] = feature_df['unique_id'].astype("int")

train_df = pd.read_csv("train_data_v2.csv")
test_df = pd.read_csv("test_data_v2.csv")

# åˆä½µç‰¹å¾µé€²å»
train_df = train_df.merge(feature_df, on='unique_id', how='left')
test_df = test_df.merge(feature_df, on='unique_id', how='left')

train_df.to_csv("train_data_v2.csv", index = None)
test_df.to_csv("test_data_v2.csv", index = None)





# ç‰¹å¾µæ¬„ä½ï¼ˆæ’é™¤ç›®æ¨™èˆ‡ idï¼‰
exclude_cols = ['unique_id', 'gender', 'hold racket handed', 'play years', 'level']
feature_cols = [c for c in train_df.columns if c not in exclude_cols]

# è¨­å®šç›®æ¨™æ¬„ä½èˆ‡å°æ‡‰é¡åˆ¥æ•¸
targets = {
    'gender': 2,
    'hold racket handed': 2,
    'play years': 3,
    'level': 4,
}

# å„²å­˜é æ¸¬çµæœ
predictions = pd.DataFrame({'unique_id': test_df['unique_id']})

param_grid = {
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'gamma': [0, 0.1, 0.3],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200, 300]
}

cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# å°æ¯å€‹ç›®æ¨™åšé æ¸¬
for target, n_classes in targets.items():
    print(f"\nğŸ” Target: {target}")
    y_train = train_df[target].copy()
    X_train = train_df[feature_cols]
    X_test = test_df[feature_cols]

    # é¡åˆ¥ç·¨ç¢¼èª¿æ•´
    if n_classes == 2:
        y_train = y_train - 1
    

    if target == 'level':
        y_train = y_train - 2
        

    class_counts = Counter(y_train)

    # å›ºå®šåƒæ•¸
    base_params = {
        'objective': 'multi:softprob' if n_classes > 2 else 'binary:logistic',
        'eval_metric': 'mlogloss',
        'tree_method': 'hist',
    }
    if n_classes > 2:
        base_params['num_class'] = n_classes
        scoring = micro_auc_scorer
    else:
        # è‡ªå‹•åŠ  scale_pos_weightï¼ˆåƒ… binaryï¼‰  
        scoring = 'roc_auc'
        neg_class = min(class_counts, key=class_counts.get)
        pos_class = max(class_counts, key=class_counts.get)
        base_params['scale_pos_weight'] = class_counts[neg_class] / class_counts[pos_class]
        print(f"âš–ï¸  scale_pos_weight = {base_params['scale_pos_weight']:.2f}")

    # å»ºç«‹ base model
    base_model = xgb.XGBClassifier(**base_params)

    # Grid Search
    # éš¨æ©Ÿæœå°‹è¶…åƒæ•¸
    search = RandomizedSearchCV(
        base_model,
        param_distributions=param_grid,
        n_iter=50,
        scoring=scoring,
        cv=cv,
        verbose=1,
        n_jobs=-1,
        random_state=42
    )
    search.fit(X_train, y_train)

    print(f"[{target}] Best Params: {search.best_params_}")
    print(f"[{target}] Best CV Accuracy: {search.best_score_:.4f}")

    # é æ¸¬
    best_model = search.best_estimator_
    if n_classes == 2:
        prob = best_model.predict_proba(X_test)[:, 1]
        predictions[target] = prob
    else:
        prob = best_model.predict_proba(X_test)
        for i in range(n_classes):
            class_index = i + (2 if target == 'level' else 0)
            predictions[f'{target}_{class_index}'] = prob[:, i]


predictions['gender'] = 1 - predictions['gender']
predictions['hold racket handed'] = 1 - predictions['hold racket handed']

# result_df = predictions
# å››æ¨äº”å…¥æ‰€æœ‰é unique_id çš„æ¬„ä½åˆ°å°æ•¸é» 4 ä½
for col in predictions.columns:
    if col != 'unique_id':
        predictions[col] = predictions[col].round(3)

predictions.to_csv("../csv_folder/handed99_predictions.csv", index=False)
